



Bayesian optimization is an algorithm used for many black box optimization problems, such as hyperparameter optimization for machine learning models. In some settings, the black box function has a mixed variable input space that involves both numerical and categorical variables. Prior-data fitted networks (PFNs) are transformers that are trained to behave similar to Gaussian processes (GPs). They have been shown to perform well as a surrogate function in Bayesian optimization methods, offering similar performance capabilities to GPs, with reduced computational expense. However, they have not yet been applied to mixed variable settings. In this work, we train three PFNs using existing mixed variable surrogate models as priors, as well as one PFN trained on a mixture of priors. We integrate them into full mixed variable Bayesian optimization (MVBO) methods and conduct experiments on six different black box functions to assess their behavior. Findings suggest that, in MVBO settings, using trained PFNs as a surrogate function yields similar performance to that of their GP-based counterparts, while operating at a fraction of the computational expense for long optimization runs with over 500 iterations. 
