{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89768a0e",
   "metadata": {},
   "source": [
    "# PFN Comparison\n",
    "\n",
    "In this file, we compare the performance of different CoCaBO-trained PFNs. First, we compare the regression performance, then at overlap scores, then at BO performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72510972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('/Users/timothyshinners/Library/Python/3.9/lib/python/site-packages')\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import mcbo\n",
    "\n",
    "from mcbo import task_factory\n",
    "from mcbo.optimizers.bo_builder import BoBuilder, BO_ALGOS\n",
    "from mcbo.optimizers.non_bo.random_search import RandomSearch\n",
    "from mcbo.optimizers.non_bo.local_search import LocalSearch\n",
    "from mcbo.acq_funcs import acq_factory\n",
    "\n",
    "import pfns4bo\n",
    "from pfns4bo.pfn_bo_bayesmark import PFNOptimizer\n",
    "from pfns4mvbo.priors import cocabo_prior\n",
    "from pfns4mvbo.pfn_plotting import showPFNPosteriorDistributions, showPFNvsCOCABOPosteriorDistributions, showPFNvsCOCABO\n",
    "from pfns4mvbo.mvpfn_optimizer import MVPFNOptimizer\n",
    "from pfns4mvbo.pfn_acq_func import PFNAcqFunc\n",
    "from pfns4mvbo.pfn_model import PFNModel\n",
    "from pfns4mvbo.evaluation import do_regression_evaluation, compare_pfn_vs_mcbo, do_validation_experiment\n",
    "import re\n",
    "\n",
    "\n",
    "def bootstrap_ub(col):\n",
    "    # returns the lower and upper bounds of the 95% confidence interval\n",
    "    bootstrap_samples = np.random.choice(col, size=[1000, len(col)], replace=True).mean(axis=1)\n",
    "    assert bootstrap_samples.shape[0] == 1000\n",
    "    ub = np.quantile(bootstrap_samples, 0.975)\n",
    "    return ub\n",
    "\n",
    "def bootstrap_lb(col):\n",
    "    # returns the lower and upper bounds of the 95% confidence interval\n",
    "    bootstrap_samples = np.random.choice(col, size=[1000, len(col)], replace=True).mean(axis=1)\n",
    "    lb = np.quantile(bootstrap_samples, 0.025)\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61274ca6-65e4-4681-9b99-2b3c062b912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3642e5-be01-44c1-927b-c82db8794e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PFN_LIST = ['0', '1', '2', '3', '4', '5', '7', '8'] \n",
    "PFN_LIST = ['2', '5', '8']\n",
    "PFN_LIST = ['5', '12', '13', '14']\n",
    "PFN_LIST = ['5']\n",
    "for i in range(26,32):\n",
    "    PFN_LIST += [str(i)]\n",
    "\n",
    "PFN_LIST = ['5', '30', '33', '34', '32']\n",
    "PFN_LIST = ['5', '30', '32','41','51','57']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb8664-be68-4f4a-aa08-bd310933e619",
   "metadata": {},
   "source": [
    "# Loss Curves\n",
    "We compare the loss curves across different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8938664-59c9-4ad8-b2dd-530a467efe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pfn_no in PFN_LIST:\n",
    "    try:\n",
    "        pfn = torch.load(f'trained_models/pfn_cocabo_{pfn_no}.pth')\n",
    "        print(pfn_no, pfn.info)\n",
    "        loss_curve = pfn.info['loss_curve']\n",
    "        print(pfn_no, pfn.info.get('learning_rate', None), pfn.info.get('bptt', None))\n",
    "        plt.plot(np.linspace(0, 1, len(loss_curve)), loss_curve, label=pfn_no)\n",
    "    except:\n",
    "        print(f'PFN {pfn_no} has no loss curve')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623d875-8bc5-4657-8984-0d08a0a4350e",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "We assess the regression performance of our trained PFN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09730e-8377-427c-ad9f-d19e4088d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mse_raw'\n",
    "\n",
    "regression_df_list = []\n",
    "\n",
    "for i, pfn_no in enumerate(PFN_LIST):\n",
    "    regression_df_list += [pd.read_csv('evaluation_data/regression_results_'+pfn_no+'.csv')]\n",
    "    regression_df_list[i] = regression_df_list[i][regression_df_list[i]['model_name'] == 'PFN']\n",
    "    regression_df_list[i]['PFN_number'] = pfn_no\n",
    "\n",
    "cocabo_baseline = pd.read_csv('evaluation_data/regression_results_'+PFN_LIST[0]+'.csv')\n",
    "cocabo_baseline = cocabo_baseline[cocabo_baseline['model_name'] != 'PFN']\n",
    "cocabo_baseline['PFN_number'] = 'CoCaBO'\n",
    "cocabo_baseline = cocabo_baseline.reset_index(drop=True)\n",
    "\n",
    "regression_df_list += [cocabo_baseline]\n",
    "\n",
    "# check the seed is consistent across data frames\n",
    "seed = regression_df_list[0]['seed'][0]\n",
    "for df in regression_df_list:\n",
    "    assert df['seed'][0] == seed\n",
    "\n",
    "if 'CoCaBO' not in PFN_LIST:\n",
    "    PFN_LIST += ['CoCaBO']\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "ax[0,0].set_title('Loss vs Amount of Training Data, PFN')\n",
    "ax[0,0].set_xlabel('Number of Training Points')\n",
    "ax[0,0].set_ylabel('Loss')\n",
    "ax[1,0].set_title('Loss vs Number of Dimensions, PFN')\n",
    "ax[1,0].set_xlabel('Number of Dimensions')\n",
    "ax[1,0].set_ylabel('Loss')\n",
    "ax[2,0].set_title('Histogram of Losses, PFN')\n",
    "ax[2,0].set_xlabel('Loss')\n",
    "ax[0,1].set_title('Ranks')\n",
    "ax[0,1].set_xlabel('Number of Training Points')\n",
    "ax[0,1].set_ylabel('Rank')\n",
    "ax[1,1].set_title('Ranks')\n",
    "ax[1,1].set_xlabel('Number of Dimensions')\n",
    "ax[1,1].set_ylabel('Rank')\n",
    "ax[2,1].set_title('Best PFN')\n",
    "ax[2,1].set_xlabel('Number of Training Points')\n",
    "ax[2,1].set_ylabel('Number of Dimensions')\n",
    "\n",
    "for i, regression_df in enumerate(regression_df_list):\n",
    "    mean = regression_df.groupby('n_training_data')[loss].mean()\n",
    "    median = regression_df.groupby('n_training_data')[loss].median()\n",
    "    ub = regression_df.groupby('n_training_data')[loss].apply(bootstrap_ub)\n",
    "    lb = regression_df.groupby('n_training_data')[loss].apply(bootstrap_lb)\n",
    "    ax[0,0].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,0].fill_between(mean.index.get_level_values('n_training_data'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    regression_df['n_dims'] = regression_df['num_dims'] + regression_df['cat_dims']\n",
    "    mean = regression_df.groupby('n_dims')[loss].mean()\n",
    "    median = regression_df.groupby('n_dims')[loss].median()\n",
    "    ub = regression_df.groupby('n_dims')[loss].apply(bootstrap_ub)\n",
    "    lb = regression_df.groupby('n_dims')[loss].apply(bootstrap_lb)\n",
    "    ax[1,0].plot(mean, label = PFN_LIST[i])\n",
    "    ax[1,0].fill_between(mean.index.get_level_values('n_dims'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    ax[2,0].hist(regression_df[loss], alpha=0.3, bins=100, label = PFN_LIST[i])\n",
    "\n",
    "# now we make ranking plot, using the raw loss calculations\n",
    "regression_df = regression_df_list[0]\n",
    "losses = np.array([df[loss[0:3]+'_raw'] for df in regression_df_list]).T\n",
    "ranks = np.argsort(losses, axis=1)\n",
    "ranks = np.argsort(ranks)\n",
    "\n",
    "avg_ranks = np.zeros(len(PFN_LIST))\n",
    "proportion_wins = np.zeros(len(PFN_LIST))\n",
    "\n",
    "for i in range(ranks.shape[1]):\n",
    "    regression_df[PFN_LIST[i]+'_rank'] = ranks[:, i]\n",
    "    avg_ranks[i] = regression_df[PFN_LIST[i]+'_rank'].mean()\n",
    "    proportion_wins[i] = (regression_df[PFN_LIST[i]+'_rank'] == 0).mean()\n",
    "    mean = regression_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].mean()\n",
    "    ub = regression_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].apply(bootstrap_ub)\n",
    "    lb = regression_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].apply(bootstrap_lb)\n",
    "    ax[0,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,1].fill_between(mean.index.get_level_values('n_training_data'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    mean = regression_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].mean()\n",
    "    ub = regression_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].apply(bootstrap_ub)\n",
    "    lb = regression_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].apply(bootstrap_lb)\n",
    "    ax[1,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[1,1].fill_between(mean.index.get_level_values('n_dims'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    best = regression_df[regression_df[PFN_LIST[i]+'_rank'] == 0]\n",
    "    noise_x = np.random.normal(0, 1.5, best.shape[0])\n",
    "    noise_y = np.random.normal(0, 1.5, best.shape[0])\n",
    "    ax[2,1].scatter(best['n_training_data']+noise_x, best['n_dims']+noise_y, s=3, label = PFN_LIST[i])\n",
    "\n",
    "# BASELINES\n",
    "cocabo_baseline = pd.read_csv('evaluation_data/regression_results_'+PFN_LIST[0]+'.csv')\n",
    "cocabo_baseline = cocabo_baseline[cocabo_baseline['model_name'] != 'PFN']\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "print(PFN_LIST)\n",
    "print('Average Ranks', avg_ranks)\n",
    "print('Win Proportion', proportion_wins)\n",
    "\n",
    "    \n",
    "ax[0,0].legend()\n",
    "ax[1,0].legend()\n",
    "ax[2,0].legend()\n",
    "ax[0,1].legend()\n",
    "ax[2,1].legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "del regression_df_list\n",
    "del PFN_LIST[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de0650-23f4-4f33-84e9-cec1a4e54711",
   "metadata": {},
   "source": [
    "# Divergence From CoCaBO\n",
    "\n",
    "In the plots below we investigate each PFN's ability to imitate CoCaBo, using KL-divergence as a criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df277-b0e2-4f32-9564-880b5839d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 'div_raw'\n",
    "\n",
    "divergence_df_list = []\n",
    "\n",
    "for i, pfn_no in enumerate(PFN_LIST):\n",
    "    divergence_df_list += [pd.read_csv('evaluation_data/divergence_results_'+pfn_no+'.csv')]\n",
    "    divergence_df_list[i] = divergence_df_list[i][divergence_df_list[i]['model_name'] == 'PFN']\n",
    "    #divergence_df_list[i] = divergence_df_list[i][divergence_df_list[i]['task_name'] == 'ackley']\n",
    "    divergence_df_list[i]['PFN_number'] = pfn_no\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "ax[0,0].set_title('Divergence vs Amount of Training Data')\n",
    "ax[0,0].set_xlabel('Number of Training Points')\n",
    "ax[0,0].set_ylabel('Divergence')\n",
    "ax[1,0].set_title('Divergence vs Number of Dimensions')\n",
    "ax[1,0].set_xlabel('Number of Dimensions')\n",
    "ax[1,0].set_ylabel('Divergence')\n",
    "ax[2,0].set_title('Histogram of Divergence')\n",
    "ax[2,0].set_xlabel('Divergence')\n",
    "ax[0,1].set_title('Ranks')\n",
    "ax[0,1].set_xlabel('Number of Training Points')\n",
    "ax[0,1].set_ylabel('Rank')\n",
    "ax[1,1].set_title('Ranks')\n",
    "ax[1,1].set_xlabel('Number of Dimensions')\n",
    "ax[1,1].set_ylabel('Rank')\n",
    "ax[2,1].set_title('Best PFN with respect to Divergence')\n",
    "ax[2,1].set_xlabel('Number of Training Points')\n",
    "ax[2,1].set_ylabel('Number of Dimensions')\n",
    "\n",
    "for i, divergence_df in enumerate(divergence_df_list):\n",
    "    mean = divergence_df.groupby('n_training_data')[div].mean()\n",
    "    median = divergence_df.groupby('n_training_data')[div].median()\n",
    "    ub = divergence_df.groupby('n_training_data')[div].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_training_data')[div].apply(bootstrap_lb)\n",
    "    ax[0,0].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,0].fill_between(mean.index.get_level_values('n_training_data'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    divergence_df['n_dims'] = divergence_df['num_dims'] + divergence_df['cat_dims']\n",
    "    mean = divergence_df.groupby('n_dims')[div].mean()\n",
    "    median = divergence_df.groupby('n_dims')[div].median()\n",
    "    ub = divergence_df.groupby('n_dims')[div].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_dims')[div].apply(bootstrap_lb)\n",
    "    ax[1,0].plot(mean, label = PFN_LIST[i])\n",
    "    ax[1,0].fill_between(mean.index.get_level_values('n_dims'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    #divergence_df = divergence_df[divergence_df['n_training_data'] == 100]\n",
    "\n",
    "    #ax[2,0].hist(np.log(divergence_df[div]), alpha=0.3, bins=100, label = PFN_LIST[i])\n",
    "    ax[2,0].hist(divergence_df[div], alpha=0.3, bins=100, label = PFN_LIST[i])\n",
    "\n",
    "# now we make ranking plot, using the raw loss calculations\n",
    "divergence_df = divergence_df_list[0]\n",
    "losses = np.array([df['div_raw'] for df in divergence_df_list]).T\n",
    "ranks = np.argsort(losses, axis=1)\n",
    "ranks = np.argsort(ranks)\n",
    "\n",
    "avg_ranks = np.zeros(len(PFN_LIST))\n",
    "proportion_wins = np.zeros(len(PFN_LIST))\n",
    "\n",
    "for i in range(ranks.shape[1]):\n",
    "    divergence_df[PFN_LIST[i]+'_rank'] = ranks[:, i]\n",
    "    avg_ranks[i] = divergence_df[PFN_LIST[i]+'_rank'].mean()\n",
    "    proportion_wins[i] = (divergence_df[PFN_LIST[i]+'_rank'] == 0).mean()\n",
    "    mean = divergence_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].mean()\n",
    "    ub = divergence_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].apply(bootstrap_lb)\n",
    "    ax[0,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,1].fill_between(mean.index.get_level_values('n_training_data'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    mean = divergence_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].mean()\n",
    "    ub = divergence_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].apply(bootstrap_lb)\n",
    "    ax[1,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[1,1].fill_between(mean.index.get_level_values('n_dims'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    best = divergence_df[divergence_df[PFN_LIST[i]+'_rank'] == 0]\n",
    "    noise_x = np.random.normal(0, 1.5, best.shape[0])\n",
    "    noise_y = np.random.normal(0, 1.5, best.shape[0])\n",
    "    ax[2,1].scatter(best['n_training_data']+noise_x, best['n_dims']+noise_y, s=3, label = PFN_LIST[i])\n",
    "\n",
    "print(PFN_LIST)\n",
    "print('Average Ranks', avg_ranks)\n",
    "print('Win Proportion', proportion_wins)\n",
    "\n",
    "    \n",
    "ax[0,0].legend()\n",
    "ax[1,0].legend()\n",
    "ax[2,0].legend()\n",
    "ax[0,1].legend()\n",
    "ax[2,1].legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "del divergence_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067df8c-5a5c-4afd-9bd0-8129eb88d358",
   "metadata": {},
   "source": [
    "# Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb52d7b-054d-40f0-9a2b-3a71a6df45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 'overlap'\n",
    "\n",
    "PFN_LIST = ['7', '30', '33', '36', '47', '51']\n",
    "\n",
    "divergence_df_list = []\n",
    "i = 0\n",
    "for index, pfn_no in enumerate(PFN_LIST):\n",
    "    print(index)\n",
    "    try:\n",
    "        dat_frame = pd.read_csv('evaluation_data/divergence_results_'+pfn_no+'.csv')\n",
    "        if (dat_frame['task_name']=='zakharov').sum() > 0:\n",
    "            divergence_df_list += [dat_frame]\n",
    "            divergence_df_list[i] = divergence_df_list[i][divergence_df_list[i]['model_name'] == 'PFN']\n",
    "            #divergence_df_list[i] = divergence_df_list[i][divergence_df_list[i]['task_name'] == 'ackley']\n",
    "            divergence_df_list[i]['PFN_number'] = pfn_no\n",
    "            divergence_df_list[i]['overlap'] = -divergence_df_list[i]['overlap']\n",
    "            i += 1\n",
    "        else:\n",
    "            print('BAD')\n",
    "    except:\n",
    "        print('CORRUPT')\n",
    "\n",
    "del dat_frame\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "ax[0,0].set_title('Overlap vs Amount of Training Data')\n",
    "ax[0,0].set_xlabel('Number of Training Points')\n",
    "ax[0,0].set_ylabel('Overlap')\n",
    "ax[1,0].set_title('Overlap vs Number of Dimensions')\n",
    "ax[1,0].set_xlabel('Number of Dimensions')\n",
    "ax[1,0].set_ylabel('Overlap')\n",
    "ax[2,0].set_title('Histogram of Divergence')\n",
    "ax[2,0].set_xlabel('Overlap')\n",
    "ax[0,1].set_title('Ranks')\n",
    "ax[0,1].set_xlabel('Number of Training Points')\n",
    "ax[0,1].set_ylabel('Rank')\n",
    "ax[1,1].set_title('Ranks')\n",
    "ax[1,1].set_xlabel('Number of Dimensions')\n",
    "ax[1,1].set_ylabel('Rank')\n",
    "ax[2,1].set_title('Best PFN with respect to Overlap')\n",
    "ax[2,1].set_xlabel('Number of Training Points')\n",
    "ax[2,1].set_ylabel('Number of Dimensions')\n",
    "\n",
    "mean_overlaps = np.zeros(len(PFN_LIST))\n",
    "\n",
    "for i, divergence_df in enumerate(divergence_df_list):\n",
    "    mean_overlaps[i] = divergence_df['overlap'].mean()\n",
    "    mean = divergence_df.groupby('n_training_data')[div].mean()\n",
    "    median = divergence_df.groupby('n_training_data')[div].median()\n",
    "    ub = divergence_df.groupby('n_training_data')[div].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_training_data')[div].apply(bootstrap_lb)\n",
    "    ax[0,0].plot(-mean, label = PFN_LIST[i])\n",
    "    ax[0,0].fill_between(mean.index.get_level_values('n_training_data'),\n",
    "                           -lb,\n",
    "                           -ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    divergence_df['n_dims'] = divergence_df['num_dims'] + divergence_df['cat_dims']\n",
    "    mean = divergence_df.groupby('n_dims')[div].mean()\n",
    "    median = divergence_df.groupby('n_dims')[div].median()\n",
    "    ub = divergence_df.groupby('n_dims')[div].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_dims')[div].apply(bootstrap_lb)\n",
    "    ax[1,0].plot(-mean, label = PFN_LIST[i])\n",
    "    ax[1,0].fill_between(mean.index.get_level_values('n_dims'),\n",
    "                           -lb,\n",
    "                           -ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    #divergence_df = divergence_df[divergence_df['n_training_data'] == 100]\n",
    "\n",
    "    #ax[2,0].hist(np.log(divergence_df[div]), alpha=0.3, bins=100, label = PFN_LIST[i])\n",
    "    ax[2,0].hist(-divergence_df[div], alpha=0.3, bins=100, label = PFN_LIST[i])\n",
    "\n",
    "# now we make ranking plot, using the raw loss calculations\n",
    "divergence_df = divergence_df_list[0]\n",
    "losses = np.array([df['overlap'] for df in divergence_df_list]).T\n",
    "ranks = np.argsort(losses, axis=1)\n",
    "ranks = np.argsort(ranks)\n",
    "\n",
    "avg_ranks = np.zeros(len(PFN_LIST))\n",
    "proportion_wins = np.zeros(len(PFN_LIST))\n",
    "\n",
    "for i in range(ranks.shape[1]):\n",
    "    divergence_df[PFN_LIST[i]+'_rank'] = ranks[:, i]\n",
    "    avg_ranks[i] = divergence_df[PFN_LIST[i]+'_rank'].mean()\n",
    "    proportion_wins[i] = (divergence_df[PFN_LIST[i]+'_rank'] == 0).mean()\n",
    "    mean = divergence_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].mean()\n",
    "    ub = divergence_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_training_data')[PFN_LIST[i]+'_rank'].apply(bootstrap_lb)\n",
    "    ax[0,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,1].fill_between(mean.index.get_level_values('n_training_data'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    mean = divergence_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].mean()\n",
    "    ub = divergence_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].apply(bootstrap_ub)\n",
    "    lb = divergence_df.groupby('n_dims')[PFN_LIST[i]+'_rank'].apply(bootstrap_lb)\n",
    "    ax[1,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[1,1].fill_between(mean.index.get_level_values('n_dims'),\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "\n",
    "    best = divergence_df[divergence_df[PFN_LIST[i]+'_rank'] == 0]\n",
    "    noise_x = np.random.normal(0, 1.5, best.shape[0])\n",
    "    noise_y = np.random.normal(0, 1.5, best.shape[0])\n",
    "    ax[2,1].scatter(best['n_training_data']+noise_x, best['n_dims']+noise_y, s=3, label = PFN_LIST[i])\n",
    "\n",
    "print(PFN_LIST)\n",
    "print('Average Overlap', -mean_overlaps)\n",
    "print('Average Ranks', avg_ranks)\n",
    "print('Win Proportion', proportion_wins)\n",
    "\n",
    "    \n",
    "ax[0,0].legend()\n",
    "ax[1,0].legend()\n",
    "ax[2,0].legend()\n",
    "ax[0,1].legend()\n",
    "ax[2,1].legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "#del divergence_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10293833-e727-4a4a-80a2-dde334e289cb",
   "metadata": {},
   "source": [
    "# n_fake_data\n",
    "\n",
    "n_fake_data is the number of data points we draw, then fit cocabo, then use those hyperparameters to generate training data for the PFN. Here, we investigate how n_fake_data alters pfn performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f434f5b-d0dc-4994-8468-83759468866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PFN_LIST = ['3', '4', '5']\n",
    "for i in range(26,32):\n",
    "    PFN_LIST += [str(i)]\n",
    "\n",
    "PFN_LIST += ['41']\n",
    "\n",
    "n_fake_data = np.zeros(len(PFN_LIST))\n",
    "mean_overlaps = np.zeros(len(PFN_LIST))\n",
    "lb = np.zeros(len(PFN_LIST))\n",
    "ub = np.zeros(len(PFN_LIST))\n",
    "\n",
    "for i, pfn_number in enumerate(PFN_LIST):\n",
    "    pfn = torch.load('trained_models/pfn_cocabo_'+pfn_number+'.pth')\n",
    "    try:\n",
    "        n_fake_data[i] = pfn.info['training_hyperparameters']['n_fake_data']\n",
    "    except:\n",
    "        if pfn_number == '3':\n",
    "            n_fake_data[i] = 50\n",
    "        elif pfn_number == '4':\n",
    "            n_fake_data[i] = 15\n",
    "        elif pfn_number == '5':\n",
    "            n_fake_data[i] = 5\n",
    "        else:\n",
    "            raisef\n",
    "\n",
    "    df = pd.read_csv('evaluation_data/divergence_results_'+pfn_number+'.csv')\n",
    "    mean_overlaps[i] = df['overlap'].mean()\n",
    "    lb[i] = bootstrap_lb(df['overlap'])\n",
    "    ub[i] = bootstrap_ub(df['overlap'])\n",
    "\n",
    "    print(pfn_number, n_fake_data[i])\n",
    "\n",
    "print(n_fake_data, mean_overlaps)\n",
    "\n",
    "sorted_indices = np.argsort(n_fake_data)\n",
    "\n",
    "# Apply sorting to all tensors\n",
    "n_fake_data = n_fake_data[sorted_indices]\n",
    "mean_overlaps = mean_overlaps[sorted_indices]\n",
    "lb = lb[sorted_indices]\n",
    "ub = ub[sorted_indices]\n",
    "\n",
    "plt.scatter(n_fake_data, mean_overlaps)\n",
    "plt.fill_between(n_fake_data,\n",
    "                           lb,\n",
    "                           ub,\n",
    "                           alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26015b-f67c-49fe-b5b8-17bff3f128e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(divergence_df_list):\n",
    "    print(i, divergence_df_list[i]['overlap'].min(), divergence_df_list[i]['overlap'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4118f-a7c1-4257-9400-f14c4b1eb879",
   "metadata": {},
   "source": [
    "# BO Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303f839-f7d1-426a-88ea-d6c35e32fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_df_list = []\n",
    "\n",
    "for i, pfn_no in enumerate(PFN_LIST):\n",
    "    try:\n",
    "        BO_df_list += [pd.read_csv('evaluation_data/BO_results_pfn_'+pfn_no+'.csv')]\n",
    "        BO_df_list[i]['PFN_number'] = pfn_no\n",
    "        BO_df_list[i].reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "cocabo_baseline = pd.read_csv('evaluation_data/BO_results_cocabo_baseline.csv')\n",
    "cocabo_baseline['PFN_number'] = 'CoCaBO'\n",
    "cocabo_baseline = cocabo_baseline.reset_index(drop=True)\n",
    "cocabo_baseline['optimizer_name'] = 'CoCaBO_pfnAcqFunc'\n",
    "cocabo_baseline_2 = cocabo_baseline.copy(deep=True)\n",
    "cocabo_baseline_2['optimizer_name'] = 'CoCaBO_mcboAcqFunc'\n",
    "cocabo_baseline = pd.concat([cocabo_baseline, cocabo_baseline_2], axis=0, ignore_index=True)\n",
    "BO_df_list += [cocabo_baseline]\n",
    "\n",
    "random_baseline = pd.read_csv('evaluation_data/BO_results_random_baseline.csv')\n",
    "random_baseline['PFN_number'] = 'Random'\n",
    "random_baseline = random_baseline.reset_index(drop=True)\n",
    "random_baseline['optimizer_name'] = 'Random_pfnAcqFunc'\n",
    "random_baseline_2 = random_baseline.copy(deep=True)\n",
    "random_baseline_2['optimizer_name'] = 'Random_mcboAcqFunc'\n",
    "random_baseline = pd.concat([random_baseline, random_baseline_2], axis=0, ignore_index=True)\n",
    "BO_df_list += [random_baseline]\n",
    "\n",
    "if 'CoCaBO' not in PFN_LIST:\n",
    "    PFN_LIST += ['CoCaBO']\n",
    "if 'Random' not in PFN_LIST:\n",
    "    PFN_LIST += ['Random']\n",
    "\n",
    "results_full = pd.concat(BO_df_list, axis=0, ignore_index=False)\n",
    "\n",
    "results_full['task_number'] = pd.factorize(results_full['task_name'])[0]\n",
    "\n",
    "results = results_full[results_full['optimizer_name'].str.contains('pfnAcqFunc')]\n",
    "\n",
    "def rescale(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "results['best_y_scaled'] = results.groupby('task_name')['best_y'].transform(rescale)\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(8, 12))\n",
    "\n",
    "\n",
    "for i, optimizer_name in enumerate(results['optimizer_name'].unique()):\n",
    "    print('HERE', optimizer_name)\n",
    "    filtered_df = results[results['optimizer_name'] == optimizer_name]\n",
    "    mean = filtered_df.groupby('nth_guess')['best_y_scaled'].mean()\n",
    "    median = filtered_df.groupby('nth_guess')['best_y_scaled'].median()\n",
    "    ub = filtered_df.groupby('nth_guess')['best_y_scaled'].apply(bootstrap_ub)\n",
    "    lb = filtered_df.groupby('nth_guess')['best_y_scaled'].apply(bootstrap_lb)\n",
    "    ax[0,0].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,0].fill_between(mean.index.get_level_values('nth_guess'),\n",
    "                       lb,\n",
    "                       ub,\n",
    "                       alpha=0.1)\n",
    "\n",
    "  \n",
    "\n",
    "ax[0,0].set_ylabel('best_y_scaled, averaged across all runs')\n",
    "ax[0,0].set_xlabel('iteration')\n",
    "ax[0,0].set_title('Optimizer Performance, PFN Acq Func')\n",
    "ax[0,0].legend()\n",
    "\n",
    "\n",
    "#now lets do a rank plot!\n",
    "BO_df = BO_df_list[0]\n",
    "best_ys = np.array([df['best_y'] for df in BO_df_list]).T\n",
    "ranks = np.argsort(best_ys, axis=1)\n",
    "ranks = np.argsort(ranks, axis=1)\n",
    "ranks = ranks.T.flatten()\n",
    "results_full['rank'] = ranks\n",
    "\n",
    "results = results_full[results_full['optimizer_name'].str.contains('pfnAcqFunc')]\n",
    "\n",
    "for i, optimizer_name in enumerate(results['optimizer_name'].unique()):\n",
    "    rank_df = results[results['optimizer_name'] == optimizer_name]\n",
    "    print('mean rank for ', optimizer_name, rank_df['rank'].mean())\n",
    "    mean = rank_df.groupby('nth_guess')['rank'].mean()\n",
    "    median = rank_df.groupby('nth_guess')['rank'].median()\n",
    "    ub = rank_df.groupby('nth_guess')['rank'].apply(bootstrap_ub)\n",
    "    lb = rank_df.groupby('nth_guess')['rank'].apply(bootstrap_lb)\n",
    "    ax[1,0].plot(mean)\n",
    "    ax[1,0].fill_between(mean.index.get_level_values('nth_guess'),\n",
    "                       lb,\n",
    "                       ub,\n",
    "                       alpha=0.1)\n",
    "\n",
    "ax[1,0].set_title('Ranking Plot')\n",
    "ax[1,0].set_ylabel('Average Rank Across All Setups')\n",
    "ax[1,0].set_xlabel('Iteration')\n",
    "\n",
    "for i, optimizer_name in enumerate(results['optimizer_name'].unique()):\n",
    "    rank_df = results[results['optimizer_name'] == optimizer_name]\n",
    "    print(optimizer_name+' win proportion: ', (rank_df['rank'] == 0).mean())\n",
    "    rank_df = rank_df[rank_df['rank'] == 0]\n",
    "\n",
    "    noise_x = np.random.normal(0, 0.3, rank_df.shape[0])\n",
    "    noise_y = np.random.normal(0, 0.3, rank_df.shape[0])\n",
    "\n",
    "    ax[2,0].scatter(rank_df['nth_guess']+noise_x, rank_df['task_number']+noise_y, s=1, label=optimizer_name)\n",
    "\n",
    "ax[2,0].set_ylabel('task number')\n",
    "ax[2,0].set_xlabel('iteration')\n",
    "ax[2,0].legend()\n",
    "\n",
    "\n",
    "#Now we do mcbo acq func!\n",
    "results = results_full[results_full['optimizer_name'].str.contains('mcboAcqFunc')]\n",
    "\n",
    "results['best_y_scaled'] = results.groupby('task_name')['best_y'].transform(rescale)\n",
    "\n",
    "for i, optimizer_name in enumerate(results['optimizer_name'].unique()):\n",
    "    filtered_df = results[results['optimizer_name'] == optimizer_name]\n",
    "    mean = filtered_df.groupby('nth_guess')['best_y_scaled'].mean()\n",
    "    median = filtered_df.groupby('nth_guess')['best_y_scaled'].median()\n",
    "    ub = filtered_df.groupby('nth_guess')['best_y_scaled'].apply(bootstrap_ub)\n",
    "    lb = filtered_df.groupby('nth_guess')['best_y_scaled'].apply(bootstrap_lb)\n",
    "    ax[0,1].plot(mean, label = PFN_LIST[i])\n",
    "    ax[0,1].fill_between(mean.index.get_level_values('nth_guess'),\n",
    "                       lb,\n",
    "                       ub,\n",
    "                       alpha=0.1)\n",
    "\n",
    "  \n",
    "\n",
    "ax[0,1].set_ylabel('best_y_scaled, averaged across all runs')\n",
    "ax[0,1].set_xlabel('iteration')\n",
    "ax[0,1].set_title('Optimizer Performance, MCBO Acq Func')\n",
    "ax[0,1].legend()\n",
    "\n",
    "\n",
    "#now lets do a rank plot!\n",
    "BO_df = BO_df_list[0]\n",
    "best_ys = np.array([df['best_y'] for df in BO_df_list]).T\n",
    "ranks = np.argsort(best_ys, axis=1)\n",
    "ranks = np.argsort(ranks, axis=1)\n",
    "ranks = ranks.T.flatten()\n",
    "results_full['rank'] = ranks\n",
    "\n",
    "results = results_full[results_full['optimizer_name'].str.contains('mcboAcqFunc')]\n",
    "\n",
    "for i, optimizer_name in enumerate(results['optimizer_name'].unique()):\n",
    "    rank_df = results[results['optimizer_name'] == optimizer_name]\n",
    "    print('mean rank for ', optimizer_name, rank_df['rank'].mean())\n",
    "    mean = rank_df.groupby('nth_guess')['rank'].mean()\n",
    "    median = rank_df.groupby('nth_guess')['rank'].median()\n",
    "    ub = rank_df.groupby('nth_guess')['rank'].apply(bootstrap_ub)\n",
    "    lb = rank_df.groupby('nth_guess')['rank'].apply(bootstrap_lb)\n",
    "    ax[1,1].plot(mean)\n",
    "    ax[1,1].fill_between(mean.index.get_level_values('nth_guess'),\n",
    "                       lb,\n",
    "                       ub,\n",
    "                       alpha=0.1)\n",
    "\n",
    "ax[1,1].set_title('Ranking Plot')\n",
    "ax[1,1].set_ylabel('Average Rank Across All Setups')\n",
    "ax[1,1].set_xlabel('Iteration')\n",
    "\n",
    "for i, optimizer_name in enumerate(results['optimizer_name'].unique()):\n",
    "    rank_df = results[results['optimizer_name'] == optimizer_name]\n",
    "    print(optimizer_name+' win proportion: ', (rank_df['rank'] == 0).mean())\n",
    "    rank_df = rank_df[rank_df['rank'] == 0]\n",
    "\n",
    "    noise_x = np.random.normal(0, 0.3, rank_df.shape[0])\n",
    "    noise_y = np.random.normal(0, 0.3, rank_df.shape[0])\n",
    "\n",
    "    ax[2,1].scatter(rank_df['nth_guess']+noise_x, rank_df['task_number']+noise_y, s=1, label=optimizer_name)\n",
    "\n",
    "ax[2,1].set_ylabel('task number')\n",
    "ax[2,1].set_xlabel('iteration')\n",
    "ax[2,1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "del PFN_LIST[-1]\n",
    "del PFN_LIST[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f763a82-c142-401d-bcf2-c18e514f59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7ec26-79cb-4072-a08d-e6b935e96fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652e908-f450-421d-b098-26f052caec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d274298-9448-46dd-b679-7c68f5d3a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('evaluation_data/divergence_results_5.csv')\n",
    "test2 = pd.read_csv('evaluation_data/divergence_verification_results_5.csv')\n",
    "print(test1['overlap']==test2['overlap'])\n",
    "print(((test1['overlap']-test2['overlap'])**2).median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f361e-d714-467d-a98b-3b37b4ef6b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
